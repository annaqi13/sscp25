{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os,sys\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "import torch\n",
    "import shutil\n",
    "import time\n",
    "import datetime\n",
    "import importlib\n",
    "from loguru import logger\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from bivme.preprocessing.dicom.extract_cines import extract_cines\n",
    "from bivme.preprocessing.dicom.select_views import select_views\n",
    "from bivme.preprocessing.dicom.segment_views import segment_views\n",
    "from bivme.preprocessing.dicom.correct_phase_mismatch import correct_phase_mismatch\n",
    "from bivme.preprocessing.dicom.generate_contours import generate_contours\n",
    "from bivme.preprocessing.dicom.export_guidepoints import export_guidepoints\n",
    "from bivme.plotting.plot_guidepoints import generate_html # for plotting guidepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up logging\n",
    "log_level = \"DEBUG\"\n",
    "log_format = \"<green>{time:YYYY-MM-DD HH:mm:ss.SSS zz}</green> | <level>{level: <8}</level> | <yellow>Line {line: >4} ({file}):</yellow> <b>{message}</b>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-03-19 17:05:21.923\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m3\u001b[0m - \u001b[1mUsing device: cuda\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Check if GPU is available (torch)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "logger.info(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This code reads in DICOM files and generates GPFiles for personalised biventricular mesh fitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Set up directories\n",
    "All directories will be created for you except for the input_path. This should point to your DICOMs, separated into folders by case like so:\n",
    "\n",
    "    input_path\n",
    "    └─── case1\n",
    "        │─── *\n",
    "    └─── case2\n",
    "        │─── *\n",
    "    └─── ...\n",
    "\n",
    "Don't worry about preprocessing your dicoms, separating by scan type, or excluding non-cines. The pipeline should find which ones are cines and which ones aren't by checking key terms within the series descriptions. Check src/bivme/preprocessing/dicom/extract_cines.py for the list of key terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: c:\\Users\\jdil469\\Code\\biv-me\\src\\bivme\\preprocessing\\dicom\n"
     ]
    }
   ],
   "source": [
    "batch_ID = 'test' # This will serve as your output folder name. Example: 'test'\n",
    "analyst_id = 'analyst1' # Example: 'analyst1'\n",
    "input_path = '' # Path to the input DICOM folder\n",
    "processed_path = '' # Path to the processed folder, where view predictions and segmentations will be stored. This will be created upon run time.\n",
    "states_path = '' # Path to the states folder, where the logs and view predictions will be stored for reference. This will be created upon run time.\n",
    "output_path = '' # Path to the output folder, where GP files will be stored. This will be created upon run time.\n",
    "plotting_path = '' # Path to the plotting folder, where the HTML file of plotted guidepoints will be stored. This will be created upon run time.\n",
    "\n",
    "# Target path: src/bivme/preprocessing/dicom/models\n",
    "cwd = os.getcwd()\n",
    "print('Current working directory:', cwd)\n",
    "MODEL_DIR = os.path.join(cwd,'models')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.1: Choose case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "case = '' # Enter the case ID. This should be a subfolder within the input_path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = os.path.join(input_path)\n",
    "dst = os.path.join(processed_path, batch_ID)\n",
    "states = os.path.join(states_path, batch_ID)\n",
    "output = os.path.join(output_path, batch_ID)\n",
    "plotting = os.path.join(plotting_path, batch_ID)\n",
    "\n",
    "\n",
    "case_src = os.path.join(src, case)\n",
    "if not os.path.isdir(case_src):\n",
    "    logger.error(f'Case {case} not found in source folder. Please check the case ID.')\n",
    "    sys.exit()\n",
    "\n",
    "os.makedirs(dst, exist_ok=True)\n",
    "os.makedirs(states, exist_ok=True)\n",
    "os.makedirs(output, exist_ok=True)\n",
    "os.makedirs(plotting, exist_ok=True)\n",
    "\n",
    "case_dst = os.path.join(dst, case)\n",
    "\n",
    "if os.path.exists(case_dst):\n",
    "    enter = input(f'Case {case} already processed. Do you want to overwrite? (y/n): ')\n",
    "    if enter == 'y':\n",
    "        shutil.rmtree(case_dst)\n",
    "    else:\n",
    "        print(f'Change the case ID or delete the existing folder {case_dst} before proceeding.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing case: cardiohance_001\n"
     ]
    }
   ],
   "source": [
    "# Create log file to record some details\n",
    "states = os.path.join(states, case, analyst_id)\n",
    "os.makedirs(states, exist_ok=True)\n",
    "\n",
    "logger_id = logger.add(f'{states}/log_file_{datetime.datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")}.log', level=log_level, format=log_format,\n",
    "    colorize=False, backtrace=True,\n",
    "    diagnose=True)\n",
    "\n",
    "logger.info(f'Processing case: {case}')\n",
    "\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules to ensure any changes are reflected\n",
    "importlib.reload(sys.modules[extract_cines.__module__])\n",
    "from extract_cines import extract_cines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0.2: Pre-preprocess dicoms\n",
    "By default, the pipeline takes in only cardiac cine images in .dcm format. The pre-preprocessing reads in the raw dicoms and uses the series descriptions to infer which are cines and which aren't. The 'cine only' dicoms are saved to:\n",
    "\n",
    "    processed_path   \n",
    "    └───batch_ID\n",
    "        └───processed-dicoms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.info(f'Finding cines...')\n",
    "extract_cines(case_src, case_dst, logger)\n",
    "\n",
    "case_src = os.path.join(case_dst, 'processed-dicoms') # Update source directory\n",
    "\n",
    "logger.success(f'Pre-preprocessing complete. Cines extracted to {case_src}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules to ensure any changes are reflected\n",
    "importlib.reload(sys.modules[select_views.__module__])\n",
    "from select_views import select_views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: View selection\n",
    "View selection can be carried out in four main ways. \n",
    "\n",
    "The first option is the 'default' option. This involves the use of two models - one using only dicom metadata and one using only image data. Each model performs prediction separately and then predictions are combined for a final verdict. This should be the most robust option, but takes longer to complete.\n",
    "\n",
    "The second option is the 'metadata-only' option. This only uses metadata. This model can struggle with subtle distinctions between similar views (e.g. between SAX and SAX-atria) but is excellent at distinguishing between general categories of view (e.g. SAX vs LAX).\n",
    "\n",
    "The third option is the 'image-only' option. This only uses image data. This model is better all round than the metadata-only model, but can occasionally produce spurious predictions. \n",
    "\n",
    "The fourth option is the 'load' option. This loads view predictions from the states folder. This is useful if view predictions have already been made, avoiding the need to rerun prediction from scratch. It also allows for you to make and save manual corrections to the selected views. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data prepared for view prediction. 14 image series found.\n",
      "Running view predictions...\n",
      "No duplicate slice locations found.\n",
      "Multiple series classed as 3ch.\n",
      "Excluded series [15]\n",
      "View predictions for cardiohance_001:\n",
      "SAX-atria: 1 series\n",
      "SAX: 5 series\n",
      "4ch: 1 series\n",
      "3ch: 1 series\n",
      "Excluded: 1 series\n",
      "2ch: 1 series\n",
      "2ch-RT: 1 series\n",
      "LVOT: 1 series\n",
      "RVOT: 1 series\n",
      "RVOT-T: 1 series\n",
      "View selection complete.\n",
      "Number of phases: 27\n"
     ]
    }
   ],
   "source": [
    "option = 'default' # Either 'default', 'metadata-only', 'image-only', or 'load'. \n",
    "# 'default' will combine dicom metadata and image data to select the correct view. 'metadata-only' will only use metadata. 'image-only' will only use the image data. \n",
    "# 'load' will load view predictions from the states folder, if view predictions have already been made.\n",
    "\n",
    "slice_info_df, num_phases, slice_mapping = select_views(case, case_src, case_dst, MODEL_DIR, states, option, logger)\n",
    "\n",
    "logger.success(f'View selection complete.')\n",
    "logger.info(f'Number of phases: {num_phases}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually correct view predictions (optional)\n",
    "No view prediction network is perfect. You can manually correct the view predictions if needed. The view predictions will be saved in:\n",
    "\n",
    "    states_path   \n",
    "    └───batch_ID\n",
    "        └───case\n",
    "            └───analyst_id\n",
    "                |───view_predictions.csv\n",
    "\n",
    "You can visually inspect which images have been classified as which view (or excluded) here:\n",
    "\n",
    "\n",
    "    processed_path   \n",
    "    └───batch_ID\n",
    "        └───case\n",
    "            └───view-classsification\n",
    "                └───sorted\n",
    "\n",
    "If you do correct the views, make run the cell block below with the variable 'option' set to 'load'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "option = 'load' # Either 'default', 'metadata-only', 'image-only', or 'load'. \n",
    "# 'default' will combine dicom metadata and image data to select the correct view. 'metadata-only' will only use metadata. 'image-only' will only use the image data. \n",
    "# 'load' will load view predictions from the states folder, if view predictions have already been made.\n",
    "\n",
    "slice_info_df, num_phases, slice_mapping = select_views(case, case_src, case_dst, MODEL_DIR, states, option, logger)\n",
    "\n",
    "logger.success(f'View selection complete.')\n",
    "logger.info(f'Number of phases: {num_phases}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules to ensure any changes are reflected\n",
    "importlib.reload(sys.modules[segment_views.__module__])\n",
    "from segment_views import segment_views"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing SAX images to nifti files...\n",
      "\n",
      "Segmenting SAX images...\n",
      "\n",
      "*** Making predictions for SAX images ***\n",
      "There are 5 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 5 cases that I would like to predict\n",
      "\n",
      "Predicting SAX_3d_10:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:03<00:00,  3.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with SAX_3d_10\n",
      "\n",
      "Predicting SAX_3d_11:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with SAX_3d_11\n",
      "\n",
      "Predicting SAX_3d_12:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with SAX_3d_12\n",
      "\n",
      "Predicting SAX_3d_8:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with SAX_3d_8\n",
      "\n",
      "Predicting SAX_3d_9:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with SAX_3d_9\n",
      "Done with SAX\n",
      "\n",
      "Writing 2ch images to nifti files...\n",
      "\n",
      "Segmenting 2ch images...\n",
      "\n",
      "*** Making predictions for 2ch images ***\n",
      "There are 1 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 1 cases that I would like to predict\n",
      "\n",
      "Predicting 2ch_3d_16:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with 2ch_3d_16\n",
      "Done with 2ch\n",
      "\n",
      "Writing 3ch images to nifti files...\n",
      "\n",
      "Segmenting 3ch images...\n",
      "\n",
      "*** Making predictions for 3ch images ***\n",
      "There are 1 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 1 cases that I would like to predict\n",
      "\n",
      "Predicting 3ch_3d_14:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:01<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with 3ch_3d_14\n",
      "Done with 3ch\n",
      "\n",
      "Writing 4ch images to nifti files...\n",
      "\n",
      "Segmenting 4ch images...\n",
      "\n",
      "*** Making predictions for 4ch images ***\n",
      "There are 1 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 1 cases that I would like to predict\n",
      "\n",
      "Predicting 4ch_3d_13:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.69s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with 4ch_3d_13\n",
      "Done with 4ch\n",
      "\n",
      "Writing RVOT images to nifti files...\n",
      "\n",
      "Segmenting RVOT images...\n",
      "\n",
      "*** Making predictions for RVOT images ***\n",
      "There are 1 cases in the source folder\n",
      "I am process 0 out of 1 (max process ID is 0, we start counting with 0!)\n",
      "There are 1 cases that I would like to predict\n",
      "\n",
      "Predicting RVOT_3d_19:\n",
      "perform_everything_on_device: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:02<00:00,  2.35s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sending off prediction to background worker for resampling and export\n",
      "done with RVOT_3d_19\n",
      "Done with RVOT\n",
      "\n",
      "Segmentation complete. Time taken: 95.30491971969604 seconds (3d version).\n"
     ]
    }
   ],
   "source": [
    "## Segmentation\n",
    "version = '3d' # '2d' or '3d' segmentation models. 3D is recommended for better efficiency and temporal coherence across frames.\n",
    "seg_start_time = time.time()\n",
    "segment_views(case, case_dst, MODEL_DIR, slice_info_df, version, logger)\n",
    "seg_end_time = time.time()\n",
    "\n",
    "logger.success(f'Segmentation complete. Time taken: {seg_end_time-seg_start_time} seconds ({version} version).')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.1: Correct for mismatching phases (conditional)\n",
    "Often, LAX and SAX series do not have matching number of phases. In that case, we need to resample segmentations to have the same number of phases. We use the SAX series as the reference for the 'right' number of phases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules to ensure any changes are reflected\n",
    "importlib.reload(sys.modules[correct_phase_mismatch.__module__])\n",
    "from correct_phase_mismatch import correct_phase_mismatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
     ]
    }
   ],
   "source": [
    "## Resample segmentations if phases are not consistent between SAX and LAX views\n",
    "correct_phase_mismatch(case_dst, slice_info_df, num_phases, logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2.2: Review segmentations (optional)\n",
    "Images and segmentations are stored here in nifti format (.nii.gz). \n",
    "\n",
    "    processed_path   \n",
    "    └───batch_ID\n",
    "        └───case\n",
    "\n",
    "\n",
    "Hopefully it won't be necessary 99% of the time, but, if you wish, segmentations can be corrected here, and the rest of the code will incorporate those changes. \n",
    "\n",
    "By default, the contouring code (after segmentation) carries out some basic QC. All label types (except for the RV myocardium) have all but their largest components removed, so you shouldn't need to worry about fixing minor things such as removing extraneous label regions. If you find a problem with the exported contours or fitted models, it's more likely due to poor RV myo segmentation or poor segmentation around valve planes.   \n",
    "\n",
    "3D Slicer or ITKSnap are good tools for correcting segmentations. You should only change the segmentations with '3d' in the name. These are the ones that get loaded later on for contouring & exporting. Both versions of segmentation model will output these '3d' segmentation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload modules to ensure any changes are reflected\n",
    "importlib.reload(sys.modules[generate_contours.__module__])\n",
    "from generate_contours import generate_contours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Generate contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating contours for SAX slice 8...\n",
      "\n",
      "Generating contours for SAX slice 9...\n",
      "\n",
      "Generating contours for SAX slice 10...\n",
      "\n",
      "Generating contours for SAX slice 11...\n",
      "\n",
      "Generating contours for SAX slice 12...\n",
      "\n",
      "Generating contours for 2ch slice 16...\n",
      "\n",
      "Generating contours for 3ch slice 14...\n",
      "\n",
      "Generating contours for 4ch slice 13...\n",
      "\n",
      "Generating contours for RVOT slice 19...\n",
      "\n",
      "Guide points generated.\n"
     ]
    }
   ],
   "source": [
    "slice_dict = generate_contours(case, case_dst, slice_info_df, num_phases, version, logger)\n",
    "logger.success(f'Guide points generated successfully.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload to ensure any changes are reflected\n",
    "importlib.reload(sys.modules[export_guidepoints.__module__])\n",
    "from export_guidepoints import export_guidepoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Export guidepoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Export complete.\n",
      "Case cardiohance_001 complete.\n",
      "Total time taken: 102.80258440971375 seconds.\n"
     ]
    }
   ],
   "source": [
    "export_guidepoints(case, case_dst, output, slice_dict, slice_mapping)\n",
    "logger.success(f'Guide points exported successfully.')\n",
    "logger.success(f'Case {case} complete.')\n",
    "logger.info(f'Total time taken: {time.time()-start_time} seconds.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 (Optional): Plot guidepoints\n",
    "The code below will plot the guidepoints at each frame as html files, which can be viewed at:\n",
    "\n",
    "    plotting_path   \n",
    "    └───batch_ID\n",
    "        └───case\n",
    "            └───html\n",
    "\n",
    "Here's a good place to find any issues before proceeding to model fitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-02-10 13:29:38.153\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mbivme.plotting.plot_guidepoints\u001b[0m:\u001b[36mgenerate_html\u001b[0m:\u001b[36m63\u001b[0m - \u001b[1mcase: cardiohance_045\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Guidepoints plotted and saved in C:\\Users\\jdil469\\bivme-data\\fitting\\plotting\\cardiohance-lvh.\n"
     ]
    }
   ],
   "source": [
    "gp_dir = os.path.join(output, case)\n",
    "generate_html(gp_dir, out_dir=plotting, gp_suffix='', si_suffix='', frames_to_fit=[], my_logger=logger, model_path=None)\n",
    "\n",
    "logger.info(f'Guidepoints plotted and saved in {plotting}.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CAP_ABI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
